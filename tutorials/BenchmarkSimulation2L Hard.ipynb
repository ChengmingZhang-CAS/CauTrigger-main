{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===== Benchmark Script for Two-Layer Causal Simulation =====\n",
    "# Goal: Evaluate layer-wise causal feature identification\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy.stats import pearsonr, spearmanr, f_oneway, norm\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import ray\n",
    "from ray import tune\n",
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from velorama.train import *\n",
    "from velorama.utils import *\n",
    "from CauTrigger.utils import set_seed, select_features\n",
    "from CauTrigger.model import CauTrigger3L, CauTrigger2L, CauTrigger1L\n",
    "from CauTrigger.dataloaders import generate_two_layer_synthetic_data\n",
    "\n",
    "sys.path.append('../')  # Âä†ÂÖ• CauTrigger ‰∏ªÁõÆÂΩï\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "plt.rcParams['font.family'] = 'sans-serif'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_CauTrigger(adata, output_dir=None, mode=\"SHAP\", topk=30, is_log1p=False, full_input=False):\n",
    "    \"\"\"\n",
    "    Run CauTrigger in two steps:\n",
    "    Step 1: Use CauTrigger1L on layer1 (closer to Y) to get top-k causal genes.\n",
    "    Step 2: Use those genes as X_down in CauTrigger2L to score layer2 (upstream).\n",
    "    Return:\n",
    "        dict {\n",
    "            \"layer1\": DataFrame with scores (index = layer1 gene names),\n",
    "            \"layer2\": DataFrame with scores (index = layer2 gene names)\n",
    "        }\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    if is_log1p:\n",
    "        print(\"CauTrigger log1p normalization applied to adata.X, adata.obsm['layer1'], and adata.obsm['layer2']\")\n",
    "        adata.X = np.log1p(adata.X)\n",
    "        adata.obsm[\"layer1\"] = np.log1p(adata.obsm[\"layer1\"])\n",
    "        adata.obsm[\"layer2\"] = np.log1p(adata.obsm[\"layer2\"])\n",
    "        adata._log1p_applied = True\n",
    "        print(\"[INFO] log1p transformation applied to adata.obsm['layer1'] and ['layer2'].\")\n",
    "\n",
    "    # === Step 1: Score all genes ===\n",
    "    adata_step1 = adata.copy()\n",
    "\n",
    "    model_1L = CauTrigger1L(\n",
    "        adata_step1,\n",
    "        n_latent=10,\n",
    "        n_hidden=128,\n",
    "        n_layers_encoder=0,\n",
    "        n_layers_decoder=0,\n",
    "        n_layers_dpd=0,\n",
    "        dropout_rate_encoder=0.0,\n",
    "        dropout_rate_decoder=0.0,\n",
    "        dropout_rate_dpd=0.0,\n",
    "        use_batch_norm='none',\n",
    "        use_batch_norm_dpd=True,\n",
    "        decoder_linear=False,\n",
    "        dpd_linear=True,\n",
    "        init_weight=None,\n",
    "        init_thresh=0.4,\n",
    "        attention=False,\n",
    "        att_mean=False,\n",
    "    )\n",
    "    model_1L.train(max_epochs=200, stage_training=True, weight_scheme=\"sim\")\n",
    "    df_step1, _ = model_1L.get_up_feature_weights(method=mode, normalize=False, sort_by_weight=True)\n",
    "    print(\"df_step1\", df_step1.head(20))\n",
    "\n",
    "    # Step 2: select top-k genes from step1 as X_down\n",
    "    topk_genes = df_step1[\"weight\"].nlargest(topk).index.tolist()\n",
    "    all_genes = adata.var_names.tolist()\n",
    "    remaining_genes = [g for g in all_genes if g not in topk_genes]\n",
    "\n",
    "    # ÊûÑÂª∫ X_down Âíå X_upstream\n",
    "    X_down = adata[:, topk_genes].X\n",
    "    X_upstream = adata[:, remaining_genes].X\n",
    "\n",
    "    # ÊûÑÂª∫ step2 ÁöÑ AnnData\n",
    "    adata_step2 = AnnData(\n",
    "        X=X_upstream,\n",
    "        obs=adata.obs.copy(),\n",
    "        var=adata.var.loc[remaining_genes].copy(),\n",
    "        obsm={\"X_down\": X_down if not full_input else adata.X}\n",
    "    )\n",
    "\n",
    "    model_2L = CauTrigger2L(\n",
    "        adata_step2,\n",
    "        n_latent=10,\n",
    "        n_hidden=128,\n",
    "        n_layers_encoder=0,\n",
    "        n_layers_decoder=0,\n",
    "        n_layers_dpd=0,\n",
    "        dropout_rate_encoder=0.0,\n",
    "        dropout_rate_decoder=0.0,\n",
    "        dropout_rate_dpd=0.0,\n",
    "        use_batch_norm='none',\n",
    "        use_batch_norm_dpd=True,\n",
    "        decoder_linear=False,\n",
    "        dpd_linear=True,\n",
    "        init_weight=None,\n",
    "        init_thresh=0.1,\n",
    "        attention=False,\n",
    "        att_mean=False,\n",
    "    )\n",
    "    model_2L.train(max_epochs=200, stage_training=True, weight_scheme=\"sim\")\n",
    "    df_step2, _ = model_2L.get_up_feature_weights(method=mode, normalize=False, sort_by_weight=True)\n",
    "    print(\"df_step2\", df_step2.head(10))\n",
    "\n",
    "    # Mark the estimation step\n",
    "    df_step1[\"step\"] = \"step1\"\n",
    "    df_step2[\"step\"] = \"step2\"\n",
    "\n",
    "    # Amplify only top-k weights from step1\n",
    "    amplify_factor = 2.0\n",
    "    df_step1_topk = df_step1.loc[topk_genes].copy()\n",
    "    df_step1_topk[\"weight\"] *= amplify_factor\n",
    "\n",
    "    # Concatenate step1_topk and all step2 genes (unaltered)\n",
    "    df_all = pd.concat([df_step1_topk, df_step2])\n",
    "\n",
    "    # Add ground truth causal label\n",
    "    df_all[\"is_causal\"] = adata.var.loc[df_all.index, \"is_causal\"]\n",
    "\n",
    "    # Assign step labels\n",
    "    df_all[\"step\"] = [\"step1\" if g in topk_genes else \"step2\" for g in df_all.index]\n",
    "\n",
    "    # Keep selected columns and align with original gene order\n",
    "    df_all = df_all[[\"weight\", \"is_causal\", \"step\"]]\n",
    "    df_all = df_all.reindex(adata.var_names)  # Ensure consistent order\n",
    "\n",
    "    # === Construct step1 and step2 full tables for separate layer evaluation ===\n",
    "    df_step1_full = pd.DataFrame(index=adata.var_names)\n",
    "    df_step1_full[\"weight\"] = 0.0\n",
    "    df_step1_full.loc[df_step1.index, \"weight\"] = df_step1[\"weight\"]\n",
    "    df_step1_full[\"step\"] = \"step1\"\n",
    "    df_step1_full[\"is_causal\"] = 0\n",
    "    layer1_genes = adata.var_names[adata.var[\"layer\"] == \"layer1\"]\n",
    "    df_step1_full.loc[layer1_genes, \"is_causal\"] = adata.var.loc[layer1_genes, \"is_causal\"]\n",
    "\n",
    "    df_step2_full = pd.DataFrame(index=adata.var_names)\n",
    "    df_step2_full[\"weight\"] = 0.0\n",
    "    df_step2_full.loc[df_step2.index, \"weight\"] = df_step2[\"weight\"]\n",
    "    df_step2_full[\"step\"] = \"step2\"\n",
    "    df_step2_full[\"is_causal\"] = 0\n",
    "    layer2_genes = adata.var_names[adata.var[\"layer\"] == \"layer2\"]\n",
    "    df_step2_full.loc[layer2_genes, \"is_causal\"] = adata.var.loc[layer2_genes, \"is_causal\"]\n",
    "\n",
    "    # Return all outputs\n",
    "    return {\n",
    "        \"step1\": df_step1_full,\n",
    "        \"step2\": df_step2_full,\n",
    "        \"all\": df_all\n",
    "    }"
   ],
   "id": "9448d3502d9ebfb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_PC(adata, output_dir):\n",
    "    def gauss_ci_test(suff_stat, i, j, K):\n",
    "        corr_matrix = suff_stat[\"C\"]\n",
    "        n_samples = suff_stat[\"n\"]\n",
    "\n",
    "        if len(K) == 0:\n",
    "            r = corr_matrix[i, j]\n",
    "        elif len(K) == 1:\n",
    "            k = K[0]\n",
    "            r = (corr_matrix[i, j] - corr_matrix[i, k] * corr_matrix[j, k]) / math.sqrt(\n",
    "                (1 - corr_matrix[i, k] ** 2) * (1 - corr_matrix[j, k] ** 2)\n",
    "            )\n",
    "        else:\n",
    "            sub_corr = corr_matrix[np.ix_([i, j] + K, [i, j] + K)]\n",
    "            precision_matrix = np.linalg.pinv(sub_corr)\n",
    "            r = (-1 * precision_matrix[0, 1]) / math.sqrt(\n",
    "                abs(precision_matrix[0, 0] * precision_matrix[1, 1])\n",
    "            )\n",
    "\n",
    "        r = max(min(r, 0.99999), -0.99999)\n",
    "        z = 0.5 * math.log1p((2 * r) / (1 - r))\n",
    "        z_standard = z * math.sqrt(n_samples - len(K) - 3)\n",
    "        p_value = 2 * (1 - norm.cdf(abs(z_standard)))\n",
    "\n",
    "        return p_value\n",
    "\n",
    "    def get_neighbors(G, x, exclude_y):\n",
    "        return [i for i, connected in enumerate(G[x]) if connected and i != exclude_y]\n",
    "\n",
    "    def skeleton(suff_stat, alpha):\n",
    "        p_value_mat = np.zeros_like(suff_stat[\"C\"])\n",
    "        n_nodes = suff_stat[\"C\"].shape[0]\n",
    "        O = [[[] for _ in range(n_nodes)] for _ in range(n_nodes)]\n",
    "        G = [[i != j for i in range(n_nodes)] for j in range(n_nodes)]\n",
    "        pairs = [(i, j) for i in range(n_nodes) for j in range(i + 1, n_nodes)]\n",
    "\n",
    "        done = False\n",
    "        l = 0\n",
    "\n",
    "        while not done and any(any(row) for row in G):\n",
    "            done = True\n",
    "\n",
    "            for x, y in pairs:\n",
    "                if G[x][y]:\n",
    "                    neighbors = get_neighbors(G, x, y)\n",
    "                    if len(neighbors) >= l:\n",
    "                        done = False\n",
    "                        for K in combinations(neighbors, l):\n",
    "                            p_value = gauss_ci_test(suff_stat, x, y, list(K))\n",
    "                            if p_value > p_value_mat[x][y]:\n",
    "                                p_value_mat[x][y] = p_value_mat[y][x] = p_value\n",
    "                            if p_value >= alpha:\n",
    "                                G[x][y] = G[y][x] = False\n",
    "                                O[x][y] = O[y][x] = list(K)\n",
    "                                break\n",
    "            l += 1\n",
    "\n",
    "        return np.asarray(G, dtype=int), O, p_value_mat\n",
    "\n",
    "    def extend_cpdag(G, O):\n",
    "        n_nodes = G.shape[0]\n",
    "\n",
    "        def rule1(g):\n",
    "            pairs = [(i, j) for i in range(n_nodes) for j in range(n_nodes) if g[i][j] == 1 and g[j][i] == 0]\n",
    "            for i, j in pairs:\n",
    "                all_k = [k for k in range(n_nodes) if\n",
    "                         (g[j][k] == 1 and g[k][j] == 1) and (g[i][k] == 0 and g[k][i] == 0)]\n",
    "                for k in all_k:\n",
    "                    g[j][k] = 1\n",
    "                    g[k][j] = 0\n",
    "            return g\n",
    "\n",
    "        def rule2(g):\n",
    "            pairs = [(i, j) for i in range(n_nodes) for j in range(n_nodes) if g[i][j] == 1 and g[j][i] == 1]\n",
    "            for i, j in pairs:\n",
    "                all_k = [k for k in range(n_nodes) if\n",
    "                         (g[i][k] == 1 and g[k][i] == 0) and (g[k][j] == 1 and g[j][k] == 0)]\n",
    "                if len(all_k) > 0:\n",
    "                    g[i][j] = 1\n",
    "                    g[j][i] = 0\n",
    "            return g\n",
    "\n",
    "        def rule3(g):\n",
    "            pairs = [(i, j) for i in range(n_nodes) for j in range(n_nodes) if g[i][j] == 1 and g[j][i] == 1]\n",
    "            for i, j in pairs:\n",
    "                all_k = [k for k in range(n_nodes) if\n",
    "                         (g[i][k] == 1 and g[k][i] == 1) and (g[k][j] == 1 and g[j][k] == 0)]\n",
    "                if len(all_k) >= 2:\n",
    "                    for k1, k2 in combinations(all_k, 2):\n",
    "                        if g[k1][k2] == 0 and g[k2][k1] == 0:\n",
    "                            g[i][j] = 1\n",
    "                            g[j][i] = 0\n",
    "                            break\n",
    "            return g\n",
    "\n",
    "        pairs = [(i, j) for i in range(n_nodes) for j in range(n_nodes) if G[i][j] == 1]\n",
    "        for x, y in sorted(pairs, key=lambda x: (x[1], x[0])):\n",
    "            all_z = [z for z in range(n_nodes) if G[y][z] == 1 and z != x]\n",
    "            for z in all_z:\n",
    "                if G[x][z] == 0 and y not in O[x][z]:\n",
    "                    G[x][y] = G[z][y] = 1\n",
    "                    G[y][x] = G[y][z] = 0\n",
    "\n",
    "        old_G = np.zeros((n_nodes, n_nodes))\n",
    "        while not np.array_equal(old_G, G):\n",
    "            old_G = G.copy()\n",
    "            G = rule1(G)\n",
    "            G = rule2(G)\n",
    "            G = rule3(G)\n",
    "\n",
    "        return np.array(G)\n",
    "\n",
    "    def pc(suff_stat, alpha=0.5, verbose=False):\n",
    "        G, O, pvm = skeleton(suff_stat, alpha)\n",
    "        cpdag = extend_cpdag(G, O)\n",
    "        if verbose:\n",
    "            print(cpdag)\n",
    "        return cpdag, pvm\n",
    "\n",
    "    alpha = 0.05\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:  # Rough check for count data\n",
    "        X = np.log1p(X)  # Apply log1p for count data\n",
    "    y = adata.obs['labels'].values\n",
    "    data = pd.DataFrame(np.column_stack((X, y)))\n",
    "    cpdag, pvm = pc(\n",
    "        suff_stat={\"C\": data.corr().values, \"n\": data.shape[0]},\n",
    "        alpha=alpha\n",
    "    )\n",
    "    pv = pvm[:-1, -1]\n",
    "    arr = np.array(1 - pv).reshape(1, -1)  # ÈúÄË¶ÅËΩ¨Êç¢Êàê 2D\n",
    "    normalized_arr = sk_normalize(arr, norm='l1', axis=1)\n",
    "    return (normalized_arr.flatten())\n",
    "\n",
    "\n",
    "def run_VAE(adata, output_dir):\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:  # Rough check for count data\n",
    "        X = np.log1p(X)  # Apply log1p for count data\n",
    "    y = adata.obs['labels'].values\n",
    "    n_features = X.shape[1]\n",
    "    features = torch.tensor(X, dtype=torch.float32)\n",
    "    labels = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    n_hidden = 5\n",
    "    n_latent = 5\n",
    "\n",
    "    class VAE(nn.Module):\n",
    "        def __init__(self, num_features):\n",
    "            super().__init__()\n",
    "\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(num_features, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, 2 * n_latent),\n",
    "            )\n",
    "\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(n_latent, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, num_features),\n",
    "                # nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "            self.DPD = nn.Sequential(\n",
    "                nn.Linear(n_latent, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, 1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "        def reparameterize(self, mu, logvar):\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps * std + mu\n",
    "\n",
    "        def forward(self, x):\n",
    "            mu_logvar = self.encoder(x)\n",
    "            mu = mu_logvar[:, :n_latent]\n",
    "            logvar = mu_logvar[:, n_latent:]\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            y = self.DPD(z)\n",
    "            reconstructed = self.decoder(z)\n",
    "            return reconstructed, y, mu, logvar\n",
    "\n",
    "    model = VAE(n_features)\n",
    "    recon_criterion = nn.MSELoss()\n",
    "    dpd_criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    losses = []\n",
    "    re_losses = []\n",
    "    kl_losses = []\n",
    "    dpd_losses = []\n",
    "    for epoch in range(200):\n",
    "        for data, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, y_dpd, mu, logvar = model(data)\n",
    "            # reconstructed loss\n",
    "            re_loss = recon_criterion(recon_batch, data)\n",
    "            re_losses.append(re_loss.item())\n",
    "\n",
    "            # kl loss\n",
    "            kl_loss = (\n",
    "                -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / data.shape[0]\n",
    "            )\n",
    "            kl_losses.append(kl_loss.item())\n",
    "\n",
    "            # dpd loss\n",
    "            dpd_loss = dpd_criterion(y_dpd, targets)\n",
    "            dpd_losses.append(dpd_loss.item())\n",
    "\n",
    "            # total loss\n",
    "            if epoch <= 100:\n",
    "                loss = re_loss + kl_loss * 0.1 + dpd_loss * 0.1\n",
    "            else:\n",
    "                loss = re_loss + kl_loss * 0.1 + dpd_loss * 0.1\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Grad\n",
    "    features.requires_grad = True\n",
    "    _, y_prob, _, _ = model(features)\n",
    "    loss = dpd_criterion(y_prob, labels)\n",
    "    loss.backward()\n",
    "    grads = features.grad.abs()\n",
    "    grad_features_importance = grads.mean(dim=0)\n",
    "    # grad_df = var_df.copy()\n",
    "    grad_df = grad_features_importance.detach().numpy()\n",
    "    arr = np.array(grad_df).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm='l1', axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_RF(adata, output_dir):\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:  # Rough check for count data\n",
    "        X = np.log1p(X)  # Apply log1p for count data\n",
    "    # X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    y = adata.obs['labels'].values\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X, y)\n",
    "    arr = np.array(rf.feature_importances_.flatten()).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm='l1', axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_SVM(adata, output_dir):\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:  # Rough check for count data\n",
    "        X = np.log1p(X)  # Apply log1p for count data\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    y = adata.obs['labels'].values\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(X, y)\n",
    "    arr = np.array(svm.coef_.flatten()).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm='l1', axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_MI(adata, output_dir):\n",
    "    X = adata.X\n",
    "    if np.issubdtype(X.dtype, np.integer) or X.max() > 100:  # Rough check for count data\n",
    "        X = np.log1p(X)  # Apply log1p for count data\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    y = adata.obs['labels'].values\n",
    "    arr =  np.array(mutual_info_classif(X, y).flatten()).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm='l1', axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_VELORAMA(adata, output_dir):\n",
    "    target_genes = adata.var_names.tolist()\n",
    "    reg_genes = adata.var_names.tolist()\n",
    "\n",
    "    X_orig = adata.X.copy()\n",
    "    if np.issubdtype(X_orig.dtype, np.integer) or X_orig.max() > 100:  # Rough check for count data\n",
    "        X_orig = np.log1p(X_orig)  # Apply log1p for count data\n",
    "    std = X_orig.std(0)\n",
    "    std[std == 0] = 1\n",
    "    X = torch.FloatTensor(X_orig - X_orig.mean(0)) / std\n",
    "    X = X.to(torch.float32)\n",
    "\n",
    "    Y_orig = adata.X.copy()\n",
    "    std = Y_orig.std(0)\n",
    "    std[std == 0] = 1\n",
    "    Y = torch.FloatTensor(Y_orig - Y_orig.mean(0)) / std\n",
    "    Y = Y.to(torch.float32)\n",
    "\n",
    "    adata.uns['iroot'] = 0\n",
    "    sc.pp.scale(adata)\n",
    "\n",
    "    reg_target = 0\n",
    "    dynamics = 'pseudotime'\n",
    "    ptloc = 'pseudotime'\n",
    "    proba = 1\n",
    "    n_neighbors = 30\n",
    "    velo_mode = 'stochastic'\n",
    "    time_series = 0\n",
    "    n_comps = 20\n",
    "    lag = 5\n",
    "    name = 'velorama_run'\n",
    "    seed = 42\n",
    "    hidden = 32\n",
    "    penalty = 'H'\n",
    "    save_dir = output_dir\n",
    "    lam_start = -2\n",
    "    lam_end = 1\n",
    "    num_lambdas = 19\n",
    "\n",
    "    # A ÈÇªÊé•Áü©ÈòµÔºå\n",
    "    # AX AY ÁªìÊûúÊòØ‰∏Ä‰∏™ (lag √ó cells √ó genes) ÁöÑÂº†ÈáèÔºåË°®Á§∫‰∏çÂêåÊó∂Èó¥Ê≠•ÁöÑÊâ©Êï£ÁâπÂæÅÔºöËÆ°ÁÆó A * XÔºåË°®Á§∫ÈÄöËøáÈÇªÊé•Áü©Èòµ‰º†Êí≠ X„ÄÇËÆ°ÁÆó A^2 * XÔºåÂç≥Â∞Ü A ÂÜçÊ¨°‰º†Êí≠„ÄÇÊåÅÁª≠ËøõË°å lag Ê¨°ÔºåÂ≠òÂÇ® A^t * X„ÄÇ\n",
    "    A = construct_dag(adata, dynamics=dynamics, ptloc=ptloc, proba=proba,\n",
    "                      n_neighbors=n_neighbors, velo_mode=velo_mode,\n",
    "                      use_time=time_series, n_comps=n_comps)\n",
    "    A = torch.FloatTensor(A)\n",
    "    AX = calculate_diffusion_lags(A, X, lag)\n",
    "    AY = None\n",
    "\n",
    "    dir_name = '{}.seed{}.h{}.{}.lag{}.{}'.format(name, seed, hidden, penalty, lag, dynamics)\n",
    "\n",
    "    if not os.path.exists(os.path.join(save_dir, dir_name)):\n",
    "        os.makedirs(os.path.join(save_dir, dir_name), exist_ok=True)\n",
    "\n",
    "    ray.init(object_store_memory=6 * 1024 ** 3, ignore_reinit_error=True)  # ÂèØ‰ª•Ë∞ÉÂ§ß\n",
    "    # ray.init(local_mode=True)  # Â∞±ÂèØ‰ª•ËÆæÁΩÆÊñ≠ÁÇπÂï¶ÔºÅ\n",
    "\n",
    "    lam_list = np.logspace(lam_start, lam_end, num=num_lambdas).tolist()\n",
    "\n",
    "    config = {'name': name,\n",
    "              'AX': AX,\n",
    "              'AY': AY,\n",
    "              'Y': Y,\n",
    "              'seed': seed,\n",
    "              'lr': 0.01,\n",
    "              'lam': tune.grid_search(lam_list),\n",
    "              'lam_ridge': 0.0,\n",
    "              'penalty': penalty,\n",
    "              'lag': lag,\n",
    "              'hidden': [hidden],\n",
    "              'max_iter': 200,\n",
    "              'device': 'cpu',\n",
    "              'lookback': 5,\n",
    "              'check_every': 10,\n",
    "              'verbose': True,\n",
    "              'dynamics': dynamics,\n",
    "              'results_dir': save_dir,\n",
    "              'dir_name': dir_name,\n",
    "              'reg_target': reg_target}\n",
    "    resources_per_trial = {\"cpu\": 1, \"gpu\": 0, \"memory\": 1 * 1024 ** 3}  # ÂèØ‰ª•Ë∞ÉÂ§ß\n",
    "    analysis = tune.run(train_model, resources_per_trial=resources_per_trial, config=config, storage_path=save_dir)\n",
    "\n",
    "    target_dir = os.path.join(save_dir, dir_name)\n",
    "    base_dir = save_dir\n",
    "    move_files(base_dir, target_dir)\n",
    "\n",
    "    # aggregate results\n",
    "    lam_list = [np.round(lam, 4) for lam in lam_list]\n",
    "    all_lags = load_gc_interactions(name, save_dir, lam_list, hidden_dim=hidden, lag=lag, penalty=penalty, dynamics=dynamics, seed=seed, ignore_lag=False)  #ÂΩ¢Áä∂‰∏∫[lam_count, TG_count, TF_count, lag]\n",
    "\n",
    "    gc_mat = estimate_interactions(all_lags, lag=lag)  # tg_count x tf_count\n",
    "    gc_df = pd.DataFrame(gc_mat.cpu().data.numpy(), index=target_genes, columns=reg_genes)\n",
    "\n",
    "    ray.shutdown()\n",
    "    return gc_df.mean(axis=0).values\n",
    "\n",
    "\n",
    "def run_SCRIBE(adata, output_dir):\n",
    "    from Scribe.read_export import load_anndata\n",
    "    adata = adata.copy()\n",
    "    sc.pp.log1p(adata)\n",
    "    adata.uns['iroot'] = 0\n",
    "    sc.pp.scale(adata)\n",
    "    sc.pp.neighbors(adata)\n",
    "    sc.tl.dpt(adata)\n",
    "    adata.obs['dpt_groups'] = ['0' if i < adata.obs['dpt_pseudotime'].median() else '1' for i in adata.obs['dpt_pseudotime']]\n",
    "\n",
    "    model = load_anndata(adata)\n",
    "    model.rdi(delays=[1,2,3], number_of_processes=1, uniformization=False, differential_mode=False)  # dict_keys([1, 2, 3, 'MAX'])\n",
    "\n",
    "    edges = []\n",
    "    values = []\n",
    "    for id1 in adata.var_names:\n",
    "        for id2 in adata.var_names:\n",
    "            if id1 == id2: continue\n",
    "            edges.append(id1.lower() + \"\\t\" + id2.lower())\n",
    "            values.append(model.rdi_results[\"MAX\"].loc[id1, id2])\n",
    "\n",
    "    edges_values = [[edges[i], values[i]] for i in range(len(edges))]\n",
    "    df = pd.DataFrame(edges_values, columns=['Edge', 'Value'])\n",
    "    df[['Source', 'Target']] = df['Edge'].str.split('\\t', expand=True)\n",
    "    df_sorted = df[['Source', 'Target', 'Value']].sort_values(by='Value', ascending=False)\n",
    "    df_mean = df_sorted.groupby(\"Source\")[\"Value\"].mean().reset_index()\n",
    "    df_mean = df_mean.set_index(\"Source\").loc[list(adata.var_names)]\n",
    "\n",
    "    return np.array(df_mean['Value'])\n",
    "\n",
    "def run_DCI(adata, output_dir):\n",
    "    from causaldag import dci\n",
    "    from collections import Counter\n",
    "    import itertools as itr\n",
    "    import scipy\n",
    "    adata = adata.copy()\n",
    "    sc.pp.log1p(adata)\n",
    "    full_genes = adata.var_names.copy()  # ‰øùÂ≠òÂÖ®ÈõÜÈ°∫Â∫è\n",
    "    mean1 = adata[adata.obs['labels'] == 1].X.mean(axis=0)\n",
    "    mean0 = adata[adata.obs['labels'] == 0].X.mean(axis=0)\n",
    "    logfc = np.log2((mean1 + 1e-9) / (mean0 + 1e-9)).A1 if scipy.sparse.issparse(adata.X) else np.log2(\n",
    "        (mean1 + 1e-9) / (mean0 + 1e-9))\n",
    "    top_idx = np.argsort(np.abs(logfc))[::-1][:50]\n",
    "    adata = adata[:, top_idx]  # Êà™Êñ≠Âè™‰øùÁïôtop50\n",
    "\n",
    "    X1 = adata.X[adata.obs['labels'] == 0].astype(float)\n",
    "    X2 = adata.X[adata.obs['labels'] == 1].astype(float)\n",
    "    X1 += np.random.normal(0, 1e-6, size=X1.shape)\n",
    "    X2 += np.random.normal(0, 1e-6, size=X2.shape)\n",
    "    p = X1.shape[1]\n",
    "    if scipy.sparse.issparse(adata.X):\n",
    "        X_full = adata.X.toarray()\n",
    "    else:\n",
    "        X_full = adata.X\n",
    "    corr = np.corrcoef(X_full.T)\n",
    "    threshold = 0.3  # ÂèØË∞É\n",
    "    candidate_edges = [(i, j) for i in range(p) for j in range(i + 1, p) if abs(corr[i, j]) >= threshold]\n",
    "    print(f\"[INFO] Filtered candidate edges: {len(candidate_edges)}\")\n",
    "    difference_matrix = dci(X1, X2, difference_ug_method='constraint', difference_ug=candidate_edges, alpha_ug=0.05, alpha_skeleton=0.1, max_set_size=2)\n",
    "    ddag_edges = set(zip(*np.where(difference_matrix != 0)))\n",
    "    print(\"len(ddag_edges)\", len(ddag_edges))\n",
    "    count_dict = Counter([node for edge in ddag_edges for node in edge])\n",
    "    count_df = pd.DataFrame(count_dict.items(), columns=['node', 'count']).sort_values('count', ascending=False)\n",
    "    count_df['gene'] = [adata.var_names[i] for i in count_df['node']]\n",
    "    full_df = pd.DataFrame({'gene': full_genes})\n",
    "    count_df = full_df.merge(count_df[['gene', 'count']], on='gene', how='left').fillna({'count': 0})\n",
    "\n",
    "    arr = np.array(count_df['count'].values).reshape(1, -1)\n",
    "    normalized_arr = sk_normalize(arr, norm='l1', axis=1)\n",
    "    return normalized_arr.flatten()\n",
    "\n",
    "\n",
    "def run_GENIE3(adata, output_dir):\n",
    "    from GENIE3 import GENIE3, get_link_list\n",
    "    import tempfile\n",
    "    import os\n",
    "    adata = adata.copy()\n",
    "    sc.pp.log1p(adata)\n",
    "    adata1 = adata[adata.obs['labels'] == 1]\n",
    "    adata0 = adata[adata.obs['labels'] == 0]\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "    X = adata0.X\n",
    "    gene_names = list(adata.var_names)\n",
    "    VIM = GENIE3(X , gene_names=gene_names)\n",
    "    get_link_list(VIM, gene_names=gene_names, file_name=tmp_path)\n",
    "    df0 = pd.read_csv(tmp_path, sep='\\t', header=None)\n",
    "    os.remove(tmp_path)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "    X = adata1.X\n",
    "    gene_names = list(adata.var_names)\n",
    "    VIM = GENIE3(X, gene_names=gene_names)\n",
    "    get_link_list(VIM, gene_names=gene_names, file_name=tmp_path)\n",
    "    df1 = pd.read_csv(tmp_path, sep='\\t', header=None)\n",
    "    os.remove(tmp_path)\n",
    "\n",
    "    merged = pd.merge(df1, df0, on=[0, 1], suffixes=('_net1', '_net0'))\n",
    "    merged['Diff_Strength'] = abs(merged['2_net1'] - merged['2_net0'])\n",
    "    diff_network = merged[[0, 1, 'Diff_Strength']]\n",
    "    arr = diff_network.groupby(0)['Diff_Strength'].mean()\n",
    "    arr_sorted = arr.reindex(adata.var_names)\n",
    "    normalized_arr = minmax_scale(arr_sorted)\n",
    "\n",
    "    return normalized_arr"
   ],
   "id": "2c6905552cb50b24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def select_single_feature(score_array, threshold=None, topk=None):\n",
    "    \"\"\"\n",
    "    Select binary labels (1 for selected, 0 for not) from a 1D score array,\n",
    "    using either a threshold or top-k strategy.\n",
    "    \"\"\"\n",
    "    score_array = np.asarray(score_array)\n",
    "    n = len(score_array)\n",
    "\n",
    "    if topk is not None:\n",
    "        topk_indices = np.argsort(score_array)[-topk:]\n",
    "        selected = np.zeros(n, dtype=int)\n",
    "        selected[topk_indices] = 1\n",
    "        return selected\n",
    "\n",
    "    elif threshold is not None:\n",
    "        return (score_array >= threshold).astype(int)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Either threshold or topk must be specified.\")\n",
    "\n",
    "\n",
    "def calculate_metrics(weight_df, label_col=\"is_causal\", score_col=\"weight\", threshold=None, topk=None):\n",
    "    from sklearn.metrics import (roc_curve, auc, confusion_matrix, accuracy_score, matthews_corrcoef,\n",
    "                                 f1_score, precision_score, recall_score, precision_recall_curve)\n",
    "\n",
    "    true_label = weight_df[label_col].values\n",
    "    score_array = weight_df[score_col].values\n",
    "\n",
    "    # Get predicted label by top-k or threshold\n",
    "    pred_label = select_single_feature(score_array, threshold=threshold, topk=topk)\n",
    "\n",
    "    # AUROC\n",
    "    fpr, tpr, _ = roc_curve(true_label, score_array)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # AUPR\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(true_label, score_array)\n",
    "    aupr = auc(recall_curve, precision_curve)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_label, pred_label)\n",
    "    TN, FP = cm[0, 0], cm[0, 1]\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "    # Other metrics\n",
    "    acc = accuracy_score(true_label, pred_label)\n",
    "    mcc = matthews_corrcoef(true_label, pred_label)\n",
    "    precision = precision_score(true_label, pred_label, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(true_label, pred_label, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(true_label, pred_label, pos_label=1, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"AUROC\": roc_auc,\n",
    "        \"AUPR\": aupr,\n",
    "        \"F1\": f1,\n",
    "        \"ACC\": acc,\n",
    "        \"MCC\": mcc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Specificity\": specificity\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_prediction(true_label, pred_dict, topk=10):\n",
    "    \"\"\"\n",
    "    Evaluate each prediction array in pred_dict using standard metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for submethod, pred in pred_dict.items():\n",
    "        metrics = calculate_metrics(true_label, pred, topk=topk)\n",
    "        results.append({\n",
    "            'SubMethod': submethod,\n",
    "            'AUROC': metrics[0],\n",
    "            'AUPR': metrics[1],\n",
    "            'F1': metrics[2],\n",
    "            'ACC': metrics[3],\n",
    "            'MCC': metrics[4],\n",
    "            'Precision': metrics[5],\n",
    "            'Recall': metrics[6],\n",
    "            'Specificity': metrics[7],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_layerwise_metrics1(df, output_dir, causal_strength=0.4, p_zero=0.2):\n",
    "    \"\"\"\n",
    "    Generate Boxplot, Violinplot, and Barplot (mean¬±std) for each metric (AUROC, AUPR)\n",
    "    with Nature Methods publication-quality styling.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Global style settings ---\n",
    "    sns.set_theme(style=\"white\")\n",
    "    plt.rcParams.update({\n",
    "        \"font.size\": 14,\n",
    "        \"axes.labelsize\": 16,\n",
    "        \"axes.titlesize\": 18,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"figure.dpi\": 300,\n",
    "        \"savefig.dpi\": 300,\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"ps.fonttype\": 42,\n",
    "    })\n",
    "\n",
    "    # --- Standardize method names ---\n",
    "    df['Method'] = df['Method'].replace({'CauTrigger_SHAP': 'CauTrigger','VAEgrad': 'VAE'})\n",
    "\n",
    "    # --- Fix method order ---\n",
    "    method_order = ['CauTrigger', 'GENIE3', 'SCRIBE', 'PC', 'VAE', 'DCI', 'MI', 'RF', 'SVM']\n",
    "    df['Method'] = pd.Categorical(df['Method'], categories=method_order, ordered=True)\n",
    "\n",
    "    # --- Nature family color palette ---\n",
    "    nature_colors = [\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B2\", \"#CCB974\", \"#64B5CD\"]\n",
    "\n",
    "    metrics = ['AUROC', 'AUPR']\n",
    "\n",
    "    for metric in metrics:\n",
    "        # --- Boxplot with points overlay ---\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            palette=nature_colors,\n",
    "            width=0.6,\n",
    "            fliersize=0,  # ‰∏çÁîªÂºÇÂ∏∏ÂÄºÔºàÁî®ÁÇπÂõæ‰ª£ÊõøÔºâ\n",
    "            linewidth=1.5\n",
    "        )\n",
    "\n",
    "        # Add individual data points on top of boxplot\n",
    "        sns.stripplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            dodge=True,\n",
    "            palette=nature_colors,\n",
    "            alpha=0.5,\n",
    "            jitter=0.2,\n",
    "            marker='o',\n",
    "            edgecolor=\"gray\",\n",
    "            linewidth=0.5\n",
    "        )\n",
    "\n",
    "        # Remove duplicate legends caused by stripplot\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        n_layers = len(df['Layer'].unique())\n",
    "        plt.legend(handles[:n_layers], labels[:n_layers], title=\"Layer\", frameon=False, loc=\"best\")\n",
    "\n",
    "        plt.title(f\"{metric} (cs={causal_strength}, p={p_zero})\", pad=15)\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=0, ha='center')\n",
    "        plt.ylim(0, 1.05)\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_Comparison_Boxplot.pdf'))\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_Comparison_Boxplot.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # --- Violinplot ---\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.violinplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            palette=nature_colors,\n",
    "            inner=\"point\",  # Show individual points\n",
    "            cut=0,\n",
    "            scale=\"width\",  # Uniform violin width\n",
    "            bw=0.4,  # Slightly larger bandwidth (default 0.2)\n",
    "            width=0.7,\n",
    "            linewidth=1.0,\n",
    "            dodge=True,\n",
    "            saturation=0.8\n",
    "        )\n",
    "        plt.title(f\"{metric} (cs={causal_strength}, p={p_zero})\", pad=15)\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=0, ha='center')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend(title=\"Layer\", frameon=False, loc=\"best\")\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_Comparison_Violinplot.pdf'))\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_Comparison_Violinplot.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # --- Barplot (mean ¬± std) ---\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        summary_df = df.groupby([\"Method\", \"Layer\"])[metric].agg([\"mean\", \"std\"]).reset_index()\n",
    "\n",
    "        ax = sns.barplot(\n",
    "            data=summary_df,\n",
    "            x=\"Method\",\n",
    "            y=\"mean\",\n",
    "            hue=\"Layer\",\n",
    "            palette=nature_colors,\n",
    "            errorbar=None,\n",
    "            width=0.7\n",
    "        )\n",
    "\n",
    "        # Manually add error bars centered at each bar\n",
    "        patches = ax.patches\n",
    "        for patch, (_, row) in zip(patches, summary_df.iterrows()):\n",
    "            x = patch.get_x() + patch.get_width() / 2\n",
    "            y = patch.get_height()\n",
    "            yerr = row[\"std\"]\n",
    "            ax.errorbar(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                yerr=yerr,\n",
    "                fmt='none',\n",
    "                ecolor='black',\n",
    "                elinewidth=1.5,\n",
    "                capsize=4,\n",
    "                capthick=1.5\n",
    "            )\n",
    "\n",
    "        plt.ylabel(f\"{metric} (Mean ¬± SD)\")\n",
    "        plt.title(f\"{metric} (cs={causal_strength}, p={p_zero})\", pad=15)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=0, ha='center')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend(title=\"Layer\", frameon=False, loc=\"upper right\")\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_Comparison_Barplot.pdf'))\n",
    "        plt.savefig(os.path.join(output_dir, f'{metric}_Comparison_Barplot.png'))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_layerwise_metrics(df, output_dir, causal_strength=0.4, p_zero=0.2):\n",
    "    \"\"\"\n",
    "    Generate Boxplot, Violinplot, and Barplot (mean¬±std) for each metric (AUROC, AUPR)\n",
    "    with solid box color, improved clarity, and Nature Methods-compatible visuals.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "\n",
    "    sns.set_theme(style=\"white\")\n",
    "    plt.rcParams.update({\n",
    "        \"font.size\": 14,\n",
    "        \"axes.labelsize\": 16,\n",
    "        \"axes.titlesize\": 18,\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"figure.dpi\": 300,\n",
    "        \"savefig.dpi\": 300,\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"ps.fonttype\": 42,\n",
    "        \"font.family\": \"Arial\",\n",
    "    })\n",
    "\n",
    "    df = df.copy()\n",
    "    df['Method'] = df['Method'].replace({'CauTrigger_SHAP': 'CauTrigger', 'VAEgrad': 'VAE'})\n",
    "    method_order = ['CauTrigger', 'GENIE3', 'SCRIBE', 'PC', 'VAE', 'DCI', 'MI', 'RF', 'SVM']\n",
    "    df['Method'] = pd.Categorical(df['Method'], categories=method_order, ordered=True)\n",
    "    method_count = len(method_order)\n",
    "\n",
    "    layer_palette = {\n",
    "        'layer1': '#3E4A89',  # Ê∑±ËìùÁ¥´ ‚Üí ‰∏ãÊ∏∏/Èù†ËøëË°®Âûã\n",
    "        'layer2': '#74A9CF',  # ÊµÖËìù ‚Üí ‰∏äÊ∏∏/Ë∞ÉÊéß\n",
    "        'all': '#5B9BD5',  # üîÅ ÊîπÊàêÊ∏ÖÁàΩÁé∞‰ª£Ëìù\n",
    "    }\n",
    "\n",
    "    metrics = ['AUROC', 'AUPR']\n",
    "\n",
    "    for metric in metrics:\n",
    "        cs_tag = f\"cs{int(causal_strength * 100)}\"\n",
    "        p_tag = f\"p{p_zero}\"\n",
    "        metric_tag = metric.lower()\n",
    "\n",
    "        # === Boxplot (Optimized) ===\n",
    "        plt.figure(figsize=(7, 3.5))\n",
    "        ax = sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            palette=layer_palette,\n",
    "            width=0.6,\n",
    "            fliersize=0,\n",
    "            linewidth=0.8,\n",
    "            # boxprops=dict(edgecolor='#999999', linewidth=0.8),\n",
    "            # whiskerprops=dict(color='#999999', linewidth=0.8),\n",
    "            # capprops=dict(color='#999999', linewidth=0.8),\n",
    "            # medianprops=dict(color='black', linewidth=1.5)\n",
    "        )\n",
    "\n",
    "        # ÂçäÈÄèÊòéÂ°´ÂÖÖÈ¢úËâ≤ÔºåÊüîÂíåËßÜËßâ\n",
    "        for patch, (_, group) in zip(ax.patches, df.groupby([\"Method\", \"Layer\"])):\n",
    "            layer = group[\"Layer\"].iloc[0]\n",
    "            face_color = layer_palette.get(layer, \"#dddddd\")\n",
    "            patch.set_facecolor(face_color)\n",
    "            patch.set_alpha(0.9)\n",
    "\n",
    "        # Êõø‰ª£ÈªëÁÇπ‰∏∫ÁÅ∞ÁÇπÔºåÊõ¥ÊüîÂíå\n",
    "        sns.stripplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            dodge=True,\n",
    "            color='gray',\n",
    "            size=2,\n",
    "            jitter=0.25,\n",
    "            alpha=0.3,\n",
    "            edgecolor=None,\n",
    "            linewidth=0,\n",
    "            legend=False\n",
    "        )\n",
    "\n",
    "        # ÂàÜÈöîÁ∫øÊõ¥ÊµÖÊõ¥ÁªÜ\n",
    "        for i in range(1, method_count):\n",
    "            ax.axvline(i - 0.5, linestyle='--', color='lightgray', linewidth=0.3, zorder=0)\n",
    "\n",
    "        # Legend\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        unique_layers = list(dict.fromkeys(zip(labels, handles)))\n",
    "        labels, handles = zip(*unique_layers)\n",
    "        plt.legend(\n",
    "            handles,\n",
    "            labels,\n",
    "            title=\"Layer\",\n",
    "            frameon=False,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.18),\n",
    "            ncol=len(labels),\n",
    "            fontsize=11,\n",
    "            title_fontsize=11,\n",
    "            columnspacing=1.2,\n",
    "            handlelength=1.5\n",
    "        )\n",
    "\n",
    "        plt.ylabel(metric, fontsize=13)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "        plt.yticks(fontsize=11)\n",
    "        plt.ylim(0, 1.05)\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        fname = f\"{metric_tag}-boxplot-{cs_tag}-{p_tag}\"\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.pdf\"))\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # === Violinplot ===\n",
    "        plt.figure(figsize=(7, 3.5))\n",
    "        ax = sns.violinplot(\n",
    "            data=df,\n",
    "            x=\"Method\",\n",
    "            y=metric,\n",
    "            hue=\"Layer\",\n",
    "            palette=layer_palette,\n",
    "            inner=\"quartile\",\n",
    "            cut=0,\n",
    "            scale=\"width\",\n",
    "            bw=0.4,\n",
    "            width=0.7,\n",
    "            linewidth=1.0,\n",
    "            dodge=True,\n",
    "            saturation=0.8\n",
    "        )\n",
    "        for i in range(1, method_count):\n",
    "            ax.axvline(i - 0.5, linestyle='--', color='lightgray', linewidth=0.5, zorder=0)\n",
    "        plt.ylabel(metric, fontsize=13)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "        plt.yticks(fontsize=11)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend(\n",
    "            title=\"Layer\",\n",
    "            frameon=False,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.18),\n",
    "            ncol=len(set(df[\"Layer\"])),\n",
    "            fontsize=11,\n",
    "            title_fontsize=11,\n",
    "            columnspacing=1.2,\n",
    "            handlelength=1.5\n",
    "        )\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        fname = f\"{metric_tag}-violinplot-{cs_tag}-{p_tag}\"\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.pdf\"))\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # === Barplot ===\n",
    "        summary_df = df.groupby([\"Method\", \"Layer\"])[metric].agg([\"mean\", \"std\"]).reset_index()\n",
    "        plt.figure(figsize=(7, 3.5))\n",
    "        ax = sns.barplot(\n",
    "            data=summary_df,\n",
    "            x=\"Method\",\n",
    "            y=\"mean\",\n",
    "            hue=\"Layer\",\n",
    "            palette=layer_palette,\n",
    "            errorbar=None,\n",
    "            width=0.7\n",
    "        )\n",
    "        for i in range(1, method_count):\n",
    "            ax.axvline(i - 0.5, linestyle='--', color='lightgray', linewidth=0.5, zorder=0)\n",
    "        patches = ax.patches\n",
    "        for patch, (_, row) in zip(patches, summary_df.iterrows()):\n",
    "            x = patch.get_x() + patch.get_width() / 2\n",
    "            y = patch.get_height()\n",
    "            yerr = row[\"std\"]\n",
    "            ax.errorbar(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                yerr=yerr,\n",
    "                fmt='none',\n",
    "                ecolor='black',\n",
    "                elinewidth=1.5,\n",
    "                capsize=4,\n",
    "                capthick=1.5\n",
    "            )\n",
    "        max_y = summary_df[\"mean\"].max() + summary_df[\"std\"].max() + 0.05\n",
    "        plt.ylim(0, max(1.05, max_y))\n",
    "        plt.ylabel(f\"{metric} (Mean ¬± SD)\", fontsize=13)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "        plt.yticks(fontsize=11)\n",
    "        plt.legend(\n",
    "            title=\"Layer\",\n",
    "            frameon=False,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.18),\n",
    "            ncol=len(set(df[\"Layer\"])),\n",
    "            fontsize=11,\n",
    "            title_fontsize=11,\n",
    "            columnspacing=1.2,\n",
    "            handlelength=1.5\n",
    "        )\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        fname = f\"{metric_tag}-barplot-{cs_tag}-{p_tag}\"\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.pdf\"))\n",
    "        plt.savefig(os.path.join(output_dir, f\"{fname}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_aggregate_layerwise_metrics(\n",
    "    root_output_dir,\n",
    "    causal_strength_list,\n",
    "    p_zero_list,\n",
    "    spurious_mode='semi_hrc',\n",
    "    n_hidden=10,\n",
    "    activation='linear',\n",
    "    simulate_single_cell=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Ê±áÊÄªÊåáÂÆöÂèÇÊï∞ÁªÑÂêà‰∏ãÁöÑ Layerwise_Benchmark_Metrics.csvÔºåÂπ∂ÁîüÊàê AUROC / AUPR ÁöÑ 3x3 boxplot ÊÄªÂõæ„ÄÇ\n",
    "    Âõæ‰æãÁßªËá≥Ê†áÈ¢ò‰∏ãÊñπÔºåÈÄÇÈÖç Nature Methods È£éÊ†º„ÄÇ\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    sns.set_theme(style=\"white\")\n",
    "    plt.rcParams.update({\n",
    "        \"font.size\": 12,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"axes.titlesize\": 16,\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"figure.dpi\": 300,\n",
    "        \"savefig.dpi\": 300,\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"ps.fonttype\": 42,\n",
    "        \"font.family\": \"Arial\",\n",
    "    })\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for cs in causal_strength_list:\n",
    "        for pz in p_zero_list:\n",
    "            case_name = \"_\".join([\n",
    "                \"2L_unknown\",\n",
    "                spurious_mode,\n",
    "                f\"hidden{n_hidden}\",\n",
    "                activation,\n",
    "                f\"cs{int(cs * 100):02d}\",\n",
    "                f\"p{pz}\",\n",
    "                \"sc\" if simulate_single_cell else \"bulk\",\n",
    "            ])\n",
    "            csv_path = os.path.join(root_output_dir, case_name, \"Layerwise_Benchmark_Metrics.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df[\"ParamCombo\"] = f\"Causal Strength = {cs}, Sparsity = {pz}\"\n",
    "                all_dfs.append(df)\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"[WARN] No matching benchmark files found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    df['Method'] = df['Method'].replace({'CauTrigger_SHAP': 'CauTrigger', 'VAEgrad': 'VAE'})\n",
    "    df['Method'] = pd.Categorical(df['Method'], categories=[\n",
    "        'CauTrigger', 'GENIE3', 'SCRIBE', 'PC', 'VAE', 'DCI', 'MI', 'RF', 'SVM'\n",
    "    ], ordered=True)\n",
    "\n",
    "    tag_parts = [\n",
    "        \"2L_unknown\",  # üëà ÊõøÊç¢‰∏∫Â±ÇÁ∫ßÊú™Áü•ÂâçÁºÄ\n",
    "        spurious_mode,\n",
    "        f\"hidden{n_hidden}\",\n",
    "        activation,\n",
    "        \"sc\" if simulate_single_cell else \"bulk\",\n",
    "    ]\n",
    "    base_tag = \"_\".join(tag_parts)\n",
    "\n",
    "    for metric in [\"AUROC\", \"AUPR\"]:\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 12), sharey=True)\n",
    "        param_combos = sorted(df['ParamCombo'].unique())\n",
    "\n",
    "        for ax, combo in zip(axes.flatten(), param_combos):\n",
    "            subdf = df[df['ParamCombo'] == combo]\n",
    "            sns.boxplot(\n",
    "                data=subdf,\n",
    "                x=\"Method\",\n",
    "                y=metric,\n",
    "                hue=\"Layer\",\n",
    "                palette = {\"all\": \"#5B9BD5\"},  # ‚Äòall‚Äô Ëâ≤\n",
    "                ax=ax,\n",
    "                width=0.6,\n",
    "                fliersize=0,\n",
    "                linewidth=1,\n",
    "            )\n",
    "            ax.set_title(combo, fontsize=14)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.tick_params(axis='x', rotation=30)\n",
    "            ax.legend_.remove()\n",
    "            ax.grid(False)\n",
    "\n",
    "            # n_methods = df['Method'].nunique()\n",
    "            # for i in range(1, n_methods):\n",
    "            #     ax.axvline(x=i - 0.5, linestyle='--', color='lightgray', linewidth=0.6, zorder=0)\n",
    "\n",
    "        # ËÆæÁΩÆ‰∏ªÊ†áÈ¢òÔºåÂπ∂Ë∞ÉÊï¥ y ÂÄºÔºà‰∏çÂÜç‰∏∫ legend ËÖæÂá∫Á©∫Èó¥Ôºâ\n",
    "        fig.suptitle(f\"Overall {metric} across methods\", fontsize=18, y=1.02)\n",
    "\n",
    "        # Ê∑ªÂä†Áªü‰∏Ä legendÔºåÊîæÂú®Ê†áÈ¢òÊ≠£‰∏ãÊñπ\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(\n",
    "            handles, labels,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.0),\n",
    "            ncol=2,\n",
    "            frameon=False\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        out_prefix = f\"{base_tag}_boxplot_{metric}\"\n",
    "        fig.savefig(os.path.join(root_output_dir, f\"{out_prefix}.pdf\"), bbox_inches='tight')\n",
    "        fig.savefig(os.path.join(root_output_dir, f\"{out_prefix}.png\"), bbox_inches='tight')\n",
    "        print(f\"[INFO] Saved: {out_prefix}.pdf/.png\")\n",
    "\n",
    "        plt.close(fig)"
   ],
   "id": "29d7a2eaf0436aee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_benchmark(\n",
    "    algorithms,\n",
    "    data_dir,\n",
    "    output_dir,\n",
    "    n_datasets=3,\n",
    "    seed_list=None,\n",
    "    save_adata=True,\n",
    "    rerun=False,\n",
    "    **generation_args\n",
    "):\n",
    "    \"\"\"\n",
    "    Run benchmark evaluation for causal discovery methods on synthetic two-layer datasets.\n",
    "    Supports both single-layer methods and recursive multi-layer methods like CauTrigger.\n",
    "    \"\"\"\n",
    "    algorithm_functions = {\n",
    "        # CauTrigger scoring variants\n",
    "        \"CauTrigger_Model\": lambda adata, out: run_CauTrigger(adata, out, mode=\"Model\"),\n",
    "        \"CauTrigger_Grad\": lambda adata, out: run_CauTrigger(adata, out, mode=\"Grad\"),\n",
    "        \"CauTrigger_SHAP\": lambda adata, out: run_CauTrigger(adata, out, mode=\"SHAP\"),\n",
    "        \"CauTrigger_Ensemble\": lambda adata, out: run_CauTrigger(adata, out, mode=\"Ensemble\"),\n",
    "\n",
    "        # Other baseline methods\n",
    "        \"PC\": run_PC,\n",
    "        \"VAEgrad\": run_VAE,\n",
    "        \"SVM\": run_SVM,\n",
    "        \"RF\": run_RF,\n",
    "        \"MI\": run_MI,\n",
    "        \"DCI\": run_DCI,\n",
    "        # \"NLBayes\": run_NLBAYES,\n",
    "        \"GENIE3\": run_GENIE3,\n",
    "        # \"GRNBOOST2\": run_GRNBOOST2,\n",
    "        \"SCRIBE\": run_SCRIBE,\n",
    "        \"VELORAMA\": run_VELORAMA,\n",
    "    }\n",
    "\n",
    "    print(f\"[INFO] Running benchmark on {n_datasets} datasets with algorithms: {algorithms}\")\n",
    "\n",
    "    if seed_list is None:\n",
    "        seed_list = list(range(n_datasets))\n",
    "    else:\n",
    "        if len(seed_list) < n_datasets:\n",
    "            max_seed = max(seed_list)\n",
    "            seed_list += list(range(max_seed + 1, max_seed + 1 + (n_datasets - len(seed_list))))\n",
    "        elif len(seed_list) > n_datasets:\n",
    "            seed_list = seed_list[:n_datasets]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    all_results = []\n",
    "    stepwise_results = []\n",
    "\n",
    "    for i, seed in enumerate(seed_list):\n",
    "        set_seed(seed)\n",
    "        adata = generate_two_layer_synthetic_data(seed=seed, **generation_args)\n",
    "        print(f\"[INFO] Dataset {i + 1}: Generated with seed {seed}\")\n",
    "\n",
    "        if save_adata:\n",
    "            dataset_dir = os.path.join(data_dir, f'dataset{i + 1}')\n",
    "            os.makedirs(dataset_dir, exist_ok=True)\n",
    "            adata.write(os.path.join(dataset_dir, 'adata.h5ad'))\n",
    "\n",
    "        for algo in algorithms:\n",
    "            assert algo in algorithm_functions, f\"[ERROR] Algorithm '{algo}' not registered in algorithm_functions!\"\n",
    "\n",
    "            if algo.startswith(\"CauTrigger\"):\n",
    "                layer_name = \"all\"\n",
    "                weight_path = os.path.join(output_dir, f'weights_dataset{i}_{layer_name}_{algo}.csv')\n",
    "\n",
    "                if not rerun and os.path.exists(weight_path):\n",
    "                    print(f\"[INFO] {algo} on {layer_name} (Dataset {i + 1}) already exists. Loading...\")\n",
    "                    weight_df = pd.read_csv(weight_path, index_col=0)\n",
    "                    metrics = calculate_metrics(weight_df, score_col=\"weight\", label_col=\"is_causal\", topk=20)\n",
    "                    row = {\n",
    "                        \"Method\": algo,\n",
    "                        \"Layer\": layer_name,\n",
    "                        \"Dataset\": i,\n",
    "                        \"Seed\": seed,\n",
    "                        \"ScoreType\": \"weight\",\n",
    "                        **metrics\n",
    "                    }\n",
    "                    all_results.append(row)\n",
    "                    continue\n",
    "\n",
    "                print(f\"[INFO] Evaluating {algo} on {layer_name} (Dataset {i + 1})...\")\n",
    "                func = algorithm_functions[algo]\n",
    "                pred_dict = func(adata, output_dir)\n",
    "\n",
    "                # Save and evaluate each step's result: step1, step2, all\n",
    "                for step_key in [\"step1\", \"step2\", \"all\"]:\n",
    "                    df = pred_dict[step_key]\n",
    "                    assert \"is_causal\" in df.columns, f\"[ERROR] Missing 'is_causal' in {step_key} result.\"\n",
    "\n",
    "                    step_weight_path = os.path.join(output_dir, f'weights_dataset{i}_{step_key}_{algo}.csv')\n",
    "                    df.to_csv(step_weight_path)\n",
    "                    print(f\"[INFO] Saved {step_key} weights to: {step_weight_path}\")\n",
    "\n",
    "                    score_type = f\"weight_{step_key}\" if step_key != \"all\" else \"weight\"\n",
    "                    metrics = calculate_metrics(df, score_col=\"weight\", label_col=\"is_causal\", topk=20)\n",
    "                    row = {\n",
    "                        \"Method\": algo,\n",
    "                        \"Layer\": step_key,\n",
    "                        \"Dataset\": i,\n",
    "                        \"Seed\": seed,\n",
    "                        \"ScoreType\": score_type,\n",
    "                        **metrics\n",
    "                    }\n",
    "\n",
    "                    if step_key == \"all\":\n",
    "                        all_results.append(row)\n",
    "                    else:\n",
    "                        stepwise_results.append(row)\n",
    "\n",
    "            else:\n",
    "                layer_name = 'all'\n",
    "                sub_adata = adata.copy()\n",
    "\n",
    "                weight_path = os.path.join(output_dir, f'weights_dataset{i}_{layer_name}_{algo}.csv')\n",
    "                if not rerun and os.path.exists(weight_path):\n",
    "                # if algo != \"DCI\" and not rerun and os.path.exists(weight_path):\n",
    "\n",
    "                    print(f\"[INFO] {algo} on {layer_name} (Dataset {i + 1}) already exists. Loading...\")\n",
    "                    weight_df = pd.read_csv(weight_path, index_col=0)\n",
    "                    metrics = calculate_metrics(weight_df, score_col=\"weight\", label_col=\"is_causal\", topk=20)\n",
    "                    row = {\n",
    "                        \"Method\": algo,\n",
    "                        \"Layer\": layer_name,\n",
    "                        \"Dataset\": i,\n",
    "                        \"Seed\": seed,\n",
    "                        \"ScoreType\": \"weight\",\n",
    "                        **metrics\n",
    "                    }\n",
    "                    all_results.append(row)\n",
    "                    continue\n",
    "\n",
    "                print(f\"[INFO] Evaluating {algo} on {layer_name} (Dataset {i + 1})...\")\n",
    "                func = algorithm_functions[algo]\n",
    "                pred = func(sub_adata, output_dir)\n",
    "\n",
    "                pred_dict = {\"weight\": pred} if not isinstance(pred, dict) else pred\n",
    "                weight_df = pd.DataFrame(pred_dict, index=sub_adata.var_names)\n",
    "                weight_df[\"is_causal\"] = sub_adata.var[\"is_causal\"].values\n",
    "\n",
    "                weight_df.to_csv(weight_path)\n",
    "                print(f\"[INFO] Saved weights to: {weight_path}\")\n",
    "\n",
    "                metrics = calculate_metrics(weight_df, score_col=\"weight\", label_col=\"is_causal\", topk=20)\n",
    "                row = {\n",
    "                    \"Method\": algo,\n",
    "                    \"Layer\": layer_name,\n",
    "                    \"Dataset\": i,\n",
    "                    \"Seed\": seed,\n",
    "                    \"ScoreType\": \"weight\",\n",
    "                    **metrics\n",
    "                }\n",
    "                all_results.append(row)\n",
    "\n",
    "        del adata\n",
    "        gc.collect()\n",
    "\n",
    "    # Save final metrics\n",
    "    df = pd.DataFrame(all_results)\n",
    "    metrics_path = os.path.join(output_dir, 'Layerwise_Benchmark_Metrics.csv')\n",
    "    df.to_csv(metrics_path, index=False)\n",
    "    print(f\"[INFO] Saved final evaluation to: {metrics_path}\")\n",
    "\n",
    "    # Draw plots\n",
    "    plot_layerwise_metrics(\n",
    "        df,\n",
    "        output_dir,\n",
    "        causal_strength=generation_args.get(\"causal_strength\", 0.4),\n",
    "        p_zero=generation_args.get(\"p_zero\", 0.2)\n",
    "    )"
   ],
   "id": "d41eb6b429d17ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compare_known_vs_unknown_layers(\n",
    "    base_dir_unknown,\n",
    "    base_dir_known,\n",
    "    method=\"CauTrigger_SHAP\",\n",
    "    topk=20,\n",
    "    n_datasets=10,\n",
    "    output_path=None\n",
    "):\n",
    "    records = []\n",
    "    for i in range(n_datasets):\n",
    "        for u_layer, k_layer in [(\"step1\", \"layer1\"), (\"step2\", \"layer2\")]:\n",
    "            path_u = os.path.join(base_dir_unknown, f\"weights_dataset{i}_{u_layer}_{method}.csv\")\n",
    "            path_k = os.path.join(base_dir_known, f\"weights_dataset{i}_{k_layer}_{method}.csv\")\n",
    "\n",
    "            if not (os.path.exists(path_u) and os.path.exists(path_k)):\n",
    "                print(f\"[WARN] Missing file for dataset {i}, layer {u_layer} or {k_layer}\")\n",
    "                continue\n",
    "\n",
    "            df_u = pd.read_csv(path_u, index_col=0)\n",
    "            df_k = pd.read_csv(path_k, index_col=0)\n",
    "\n",
    "            m_u = calculate_metrics(df_u, score_col=\"weight\", label_col=\"is_causal\", topk=topk)\n",
    "            m_k = calculate_metrics(df_k, score_col=\"weight\", label_col=\"is_causal\", topk=topk)\n",
    "\n",
    "            records.append({\"Dataset\": i, \"Layer\": u_layer, \"Mode\": \"unknown\", **m_u})\n",
    "            records.append({\"Dataset\": i, \"Layer\": k_layer, \"Mode\": \"known\", **m_k})\n",
    "\n",
    "    df_compare = pd.DataFrame(records)\n",
    "    if output_path:\n",
    "        df_compare.to_csv(output_path, index=False)\n",
    "    return df_compare\n",
    "\n",
    "def plot_hierarchy_comparison(df, output_dir, metric=\"AUROC\", cs=None, p_zero=None):\n",
    "    \"\"\"\n",
    "    Compare inferred (pseudo) vs prior (true) layer-level performance using boxplot.\n",
    "    Outputs one plot per metric, with styling aligned to Nature Methods.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "\n",
    "    # === Global style ===\n",
    "    sns.set_theme(style=\"white\")\n",
    "    plt.rcParams.update({\n",
    "        \"font.size\": 13,\n",
    "        \"axes.labelsize\": 13,\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"figure.dpi\": 300,\n",
    "        \"savefig.dpi\": 300,\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"ps.fonttype\": 42,\n",
    "        \"font.family\": \"Arial\"\n",
    "    })\n",
    "\n",
    "    # === Preprocess DataFrame ===\n",
    "    df = df.copy()\n",
    "    df[\"Layer\"] = df[\"Layer\"].replace({\n",
    "        \"step1\": \"Layer 1\",\n",
    "        \"layer1\": \"Layer 1\",\n",
    "        \"step2\": \"Layer 2\",\n",
    "        \"layer2\": \"Layer 2\"\n",
    "    })\n",
    "    df[\"Layer\"] = pd.Categorical(df[\"Layer\"], categories=[\"Layer 1\", \"Layer 2\"], ordered=True)\n",
    "\n",
    "    df[\"Setting\"] = df[\"Mode\"].replace({\n",
    "        \"unknown\": \"inferred\",\n",
    "        \"known\": \"known\"\n",
    "    })\n",
    "    df[\"Setting\"] = pd.Categorical(df[\"Setting\"], categories=[\"known\", \"inferred\"], ordered=True)\n",
    "\n",
    "    setting_palette = {\n",
    "        \"inferred\": \"#2C2C79\",  # Ê∑±ËìùÁ¥´\n",
    "        \"known\": \"#9DC3E6\"      # ÊµÖËìù\n",
    "    }\n",
    "\n",
    "    # === Plot ===\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    ax = sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"Layer\",\n",
    "        y=metric,\n",
    "        hue=\"Setting\",\n",
    "        palette=setting_palette,\n",
    "        width=0.6,\n",
    "        fliersize=0,\n",
    "        gap=0.15,  # hue box ‰πãÈó¥ÁïôÁ©∫\n",
    "        linewidth=1\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        data=df,\n",
    "        x=\"Layer\",\n",
    "        y=metric,\n",
    "        hue=\"Setting\",\n",
    "        dodge=True,\n",
    "        color=\"gray\",\n",
    "        alpha=0.5,\n",
    "        size=4,\n",
    "        jitter=0.2,\n",
    "        edgecolor=\"none\",\n",
    "        linewidth=0,\n",
    "        legend=False  # Èò≤Ê≠¢ legend ÈáçÂ§ç\n",
    "    )\n",
    "\n",
    "    # === Adjust legend ===\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles[:2], labels[:2],\n",
    "        title=None,\n",
    "        frameon=False,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.12),\n",
    "        ncol=2,\n",
    "        handlelength=1.5,\n",
    "        columnspacing=1.5\n",
    "    )\n",
    "\n",
    "    # === Plot aesthetics ===\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === Save ===\n",
    "    cs_tag = f\"cs{int(cs * 100)}\" if cs is not None else \"\"\n",
    "    p_tag = f\"p{p_zero}\" if p_zero is not None else \"\"\n",
    "    fname = f\"{metric}_HierarchyComparison_{cs_tag}_{p_tag}\".strip(\"_\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"{fname}.pdf\"), bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(output_dir, f\"{fname}.png\"), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Saved {fname}.pdf/png to: {output_dir}\")\n",
    "\n",
    "\n",
    "def plot_grid_hierarchy_comparison_boxplot(\n",
    "    root_output_dir,\n",
    "    causal_strength_list,\n",
    "    p_zero_list,\n",
    "    metric=\"AUROC\",\n",
    "    spurious_mode=\"semi_hrc\",\n",
    "    n_hidden=10,\n",
    "    activation=\"linear\",\n",
    "    simulate_single_cell=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Ê±áÊÄªÂêÑÂèÇÊï∞ÁªÑÂêà‰∏ã compare_known_unknown.csv Êñá‰ª∂ÔºåÂπ∂Áîª 3x3 ‰πùÂÆ´Ê†ºÂõæÔºåÊØîËæÉÂÖàÈ™å vs Êé®Êñ≠ÁöÑÂ±ÇÁ∫ßË°®Áé∞„ÄÇ\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    sns.set_theme(style=\"white\")\n",
    "    plt.rcParams.update({\n",
    "        \"font.size\": 12,\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"axes.titlesize\": 16,\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"legend.fontsize\": 14,\n",
    "        \"figure.dpi\": 300,\n",
    "        \"savefig.dpi\": 300,\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"ps.fonttype\": 42,\n",
    "        \"font.family\": \"Arial\",\n",
    "    })\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for cs in causal_strength_list:\n",
    "        for pz in p_zero_list:\n",
    "            tag_parts = [\n",
    "                \"2L_unknown\",\n",
    "                spurious_mode,\n",
    "                f\"hidden{n_hidden}\",\n",
    "                activation,\n",
    "                f\"cs{int(cs * 100):02d}\",\n",
    "                f\"p{pz}\",\n",
    "                \"sc\" if simulate_single_cell else \"bulk\",\n",
    "            ]\n",
    "            case_name = \"_\".join(tag_parts)\n",
    "            csv_path = os.path.join(root_output_dir, case_name, \"compare_known_unknown.csv\")\n",
    "\n",
    "            if os.path.exists(csv_path):\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df[\"ParamCombo\"] = f\"Causal Strength = {cs}, Sparsity = {pz}\"\n",
    "                all_dfs.append(df)\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"[WARN] No compare_known_unknown.csv files found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    df[\"Layer\"] = df[\"Layer\"].replace({\n",
    "        \"step1\": \"Layer 1\",\n",
    "        \"layer1\": \"Layer 1\",\n",
    "        \"step2\": \"Layer 2\",\n",
    "        \"layer2\": \"Layer 2\"\n",
    "    })\n",
    "    df[\"Layer\"] = pd.Categorical(df[\"Layer\"], categories=[\"Layer 1\", \"Layer 2\"], ordered=True)\n",
    "\n",
    "    df[\"Setting\"] = df[\"Mode\"].replace({\n",
    "        \"unknown\": \"inferred\",\n",
    "        \"known\": \"known\"\n",
    "    })\n",
    "    df[\"Setting\"] = pd.Categorical(df[\"Setting\"], categories=[\"known\", \"inferred\"], ordered=True)\n",
    "\n",
    "    setting_palette = {\n",
    "        \"inferred\": \"#2C2C79\",  # Ê∑±ËìùÁ¥´\n",
    "        \"known\": \"#9DC3E6\"      # ÊµÖËìù\n",
    "    }\n",
    "\n",
    "    # === ÁªòÂõæ ===\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12), sharey=True)\n",
    "    param_combos = sorted(df['ParamCombo'].unique())\n",
    "\n",
    "    for ax, combo in zip(axes.flatten(), param_combos):\n",
    "        subdf = df[df['ParamCombo'] == combo]\n",
    "        sns.boxplot(\n",
    "            data=subdf,\n",
    "            x=\"Layer\",\n",
    "            y=metric,\n",
    "            hue=\"Setting\",\n",
    "            palette=setting_palette,\n",
    "            width=0.5,\n",
    "            fliersize=0,\n",
    "            linewidth=1,\n",
    "            gap=0.15,  # hue box ‰πãÈó¥ÁïôÁ©∫\n",
    "            # dodge=0.6, # Ë∞ÉÊï¥ÁÆ±Á∫øÂõæÁöÑÂÆΩÂ∫¶\n",
    "            ax=ax\n",
    "        )\n",
    "        sns.stripplot(\n",
    "            data=subdf,\n",
    "            x=\"Layer\",\n",
    "            y=metric,\n",
    "            hue=\"Setting\",\n",
    "            dodge=True,\n",
    "            color=\"gray\",\n",
    "            alpha=0.4,\n",
    "            size=3,\n",
    "            edgecolor=\"none\",\n",
    "            linewidth=0,\n",
    "            ax=ax,\n",
    "            legend=False\n",
    "        )\n",
    "        ax.set_title(combo, fontsize=14)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.tick_params(axis='x', labelsize=14)  # ÊéßÂà∂ Layer 1 / 2 Â≠ó‰ΩìÂ§ßÂ∞è\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.grid(False)\n",
    "        ax.legend_.remove()\n",
    "\n",
    "    fig.suptitle(f\"Layer-wise {metric}: Known vs Inferred\", fontsize=18, y=1.02)\n",
    "\n",
    "    # Áªü‰∏Ä legend\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles[:2], labels[:2],\n",
    "        title=None,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.0),\n",
    "        ncol=2,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    tag_parts = [\n",
    "        \"2L_unknown\",\n",
    "        spurious_mode,\n",
    "        f\"hidden{n_hidden}\",\n",
    "        activation,\n",
    "        \"sc\" if simulate_single_cell else \"bulk\"\n",
    "    ]\n",
    "    base_tag = \"_\".join(tag_parts)\n",
    "    fname = f\"{base_tag}_PriorVsInferred_{metric}\"\n",
    "\n",
    "    fig.savefig(os.path.join(root_output_dir, f\"{fname}.pdf\"), bbox_inches='tight')\n",
    "    fig.savefig(os.path.join(root_output_dir, f\"{fname}.png\"), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"[INFO] Saved: {fname}.pdf/.png\")"
   ],
   "id": "a6a9937dbdf3a95a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    BASE_DIR = '/mnt/e/Project_Research/CauTrigger_Project/CauTrigger-master'\n",
    "    case_dir = os.path.join(BASE_DIR, 'simulation')\n",
    "\n",
    "    # ==== Âõ∫ÂÆöÂèÇÊï∞ ====\n",
    "    n_datasets = 10\n",
    "    spurious_mode = 'semi_hrc'\n",
    "    n_hidden = 10\n",
    "    activation = 'linear'\n",
    "    simulate_single_cell = True\n",
    "\n",
    "    # algorithms = ['CauTrigger_SHAP', 'SVM', 'RF', 'MI']\n",
    "    algorithms = ['CauTrigger_SHAP', 'GENIE3', 'SCRIBE', 'PC', 'VAEgrad','DCI', 'MI', 'RF', 'SVM']\n",
    "\n",
    "    # ==== ‰Ω†ÊÉ≥Êâ´ÁöÑÂèÇÊï∞ËåÉÂõ¥ ====\n",
    "    causal_strength_list = [0.3, 0.4, 0.5, 0.6, 0.7]  # Âõ†ÊûúÂº∫Â∫¶Ôºö‰Ωé„ÄÅ‰∏≠„ÄÅÈ´ò\n",
    "    p_zero_list = [0.1, 0.3, 0.5, 0.7]  # Á®ÄÁñèÂ∫¶Ôºö‰Ωé„ÄÅ‰∏≠„ÄÅÈ´ò\n",
    "    causal_strength_list = [0.3, 0.4, 0.5]  # Âõ†ÊûúÂº∫Â∫¶Ôºö‰Ωé„ÄÅ‰∏≠„ÄÅÈ´ò\n",
    "    p_zero_list = [0.3, 0.5, 0.7]  # Á®ÄÁñèÂ∫¶Ôºö‰Ωé„ÄÅ‰∏≠„ÄÅÈ´ò\n",
    "\n",
    "    for causal_strength in causal_strength_list:\n",
    "        for p_zero in p_zero_list:\n",
    "\n",
    "            # ==== Âä®ÊÄÅÁîüÊàê‰∏ÄÁªÑÂèÇÊï∞ ====\n",
    "            generation_args = dict(\n",
    "                spurious_mode=spurious_mode,\n",
    "                n_hidden=n_hidden,\n",
    "                activation=activation,\n",
    "                causal_strength=causal_strength,\n",
    "                p_zero=p_zero,\n",
    "                simulate_single_cell=simulate_single_cell,\n",
    "            )\n",
    "\n",
    "            # ==== Âä®ÊÄÅÁîüÊàê case_name ====\n",
    "            tag_parts = [\n",
    "                \"2L_unknown\",\n",
    "                spurious_mode,\n",
    "                f\"hidden{n_hidden}\",\n",
    "                activation,\n",
    "                f\"cs{int(causal_strength * 100):02d}\",\n",
    "                f\"p{p_zero}\",\n",
    "                \"sc\" if simulate_single_cell else \"bulk\",\n",
    "            ]\n",
    "            case_name = \"_\".join(tag_parts)\n",
    "\n",
    "            data_dir = os.path.join(case_dir, 'data', case_name)\n",
    "            output_dir = os.path.join(case_dir, 'output', case_name)\n",
    "            os.makedirs(data_dir, exist_ok=True)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"=== Running: {case_name} ===\")\n",
    "\n",
    "            run_benchmark(\n",
    "                algorithms=algorithms,\n",
    "                data_dir=data_dir,\n",
    "                output_dir=output_dir,\n",
    "                n_datasets=n_datasets,\n",
    "                **generation_args\n",
    "            )\n",
    "\n",
    "            # === ÊØîËæÉÂ±ÇÁ∫ßËØÜÂà´ ===\n",
    "            case_known = case_name.replace(\"2L_unknown\", \"2L_counts\")\n",
    "            base_dir_known = os.path.join(case_dir, \"output\", case_known)\n",
    "            df_cmp = compare_known_vs_unknown_layers(\n",
    "                base_dir_unknown=output_dir,\n",
    "                base_dir_known=base_dir_known,\n",
    "                method=\"CauTrigger_SHAP\",\n",
    "                topk=20,\n",
    "                n_datasets=n_datasets,\n",
    "                output_path=os.path.join(output_dir, \"compare_known_unknown.csv\")\n",
    "            )\n",
    "            plot_hierarchy_comparison(df_cmp, output_dir, metric=\"AUROC\", cs=causal_strength, p_zero=p_zero)\n",
    "            plot_hierarchy_comparison(df_cmp, output_dir, metric=\"AUPR\", cs=causal_strength, p_zero=p_zero)\n",
    "\n",
    "    # ÊúÄÂêéÁîüÊàêÊ±áÊÄªÂõæÔºàÂè™Â§ÑÁêÜÂΩìÂâçËøôÊâπÁªÑÂêàÔºâ\n",
    "    aggregate_output_root = os.path.join(case_dir, 'output')\n",
    "    plot_aggregate_layerwise_metrics(\n",
    "        root_output_dir=os.path.join(case_dir, 'output'),\n",
    "        causal_strength_list=causal_strength_list,\n",
    "        p_zero_list=p_zero_list,\n",
    "        spurious_mode=spurious_mode,\n",
    "        n_hidden=n_hidden,\n",
    "        activation=activation,\n",
    "        simulate_single_cell=simulate_single_cell\n",
    "    )\n",
    "    plot_grid_hierarchy_comparison_boxplot(\n",
    "        root_output_dir=aggregate_output_root,\n",
    "        causal_strength_list=causal_strength_list,\n",
    "        p_zero_list=p_zero_list,\n",
    "        metric=\"AUROC\",  # Êàñ \"AUPR\"\n",
    "        spurious_mode=spurious_mode,\n",
    "        n_hidden=n_hidden,\n",
    "        activation=activation,\n",
    "        simulate_single_cell=simulate_single_cell\n",
    "    )\n",
    "    plot_grid_hierarchy_comparison_boxplot(\n",
    "        root_output_dir=aggregate_output_root,\n",
    "        causal_strength_list=causal_strength_list,\n",
    "        p_zero_list=p_zero_list,\n",
    "        metric=\"AUPR\",\n",
    "        spurious_mode=spurious_mode,\n",
    "        n_hidden=n_hidden,\n",
    "        activation=activation,\n",
    "        simulate_single_cell=simulate_single_cell\n",
    "    )"
   ],
   "id": "e6ee4c2ced507bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
